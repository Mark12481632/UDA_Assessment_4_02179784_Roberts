{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e452e005",
   "metadata": {},
   "source": [
    "# Project for UDA Assessment 4\n",
    "## Author: 02179784 (Mark Roberts)\n",
    "\n",
    "<B>Project Description:</B><BR>\n",
    "The objective of this project is to create a simple framework to test how well image classification models perform when presented with noisey images.  The idea is that models can be swapped-out/in and tested with minimal changes.\n",
    "\n",
    "In this particular implementation of the framework:\n",
    " - we use a model that has been pre-trained to classify non-noisy images with an accurancy of over 97%.<BR>\n",
    "    This model was created using the information provided in chapter 8 of [1], and is based on refining an already pretrained model.<BR>\n",
    "    This step has been omitted here as it took several hours to build!\n",
    " - all images used in the training, validation and testing of this model consisted of cat and dog images obtained from the Kaggle website: [Cats and Dog Images](www.kaggle.com/competitions/dogs-vs-cats/data).\n",
    " - testing will be done using uniform random noise, but this could be easily replaced by other noise patterns (e.g. Gaussian) if desired.  The image dataset consists of 2000 images - 1000 cat and 1000 dog images.\n",
    "\n",
    "The analysis has been split into the following steps:\n",
    " - Train the classification model using the clean black & white images.\n",
    " - Determine accuracy of the model\n",
    " - Add various levels of noise to the test images & determine accuracy of the model.\n",
    " - Conclusion / Summary\n",
    "\n",
    "\n",
    "<B>General Information:</B><BR>\n",
    " - This script was run on a Macbook Pro. (15\" 2017) with 32 GB of RAM.\n",
    " - The total run time of the Jupyter notebook was (approx) 120 seconds.\n",
    " - The Github repository for all code and datasets can be obtained by cloning the repository:\n",
    "[GitHub-Repo](git@github.com:Mark12481632/UDA_Assessment_4_02179784_Roberts.git)\n",
    " - The model used to classify the images is built on __TensorFlow__ and __Keras__.\n",
    " - **Altered images need to be saved to disk - so write permission for the notebook is required!!\n",
    "    \n",
    "<B>References:</B><BR>\n",
    "    \n",
    "    [1] - Deep Learning with Python (Francios Chollet, Manning Publications, ISBN-13: 978-1617296864)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f3c6543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-25 16:15:46.477397: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Load in the required libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Input, Model, Sequential\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3829e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-25 16:19:50.631893: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Location of non-noisy images:\n",
    "test_dataset = image_dataset_from_directory(\"./cats_vs_dogs/test\", image_size=(180, 180), batch_size=32)\n",
    "\n",
    "test_noisy_dataset = image_dataset_from_directory(\"./cats_vs_dogs/test_noisy\", image_size=(180, 180), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb9ca162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "63/63 [==============================] - 158s 3s/step - loss: 3.5702 - accuracy: 0.9755\n",
      "Test accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "test_model = load_model(\"feature_extraction_with_data_augmentation.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b366a86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
